# Context Management Strategy for LangGraph Agents

## The Problem

E2E testing agents face a unique challenge: they generate **massive amounts of data** (screenshots, HTML snapshots, detailed logs) that are essential for debugging but toxic to LLM context windows.

```
Token breakdown for a typical test run:
â”œâ”€â”€ System prompt:           ~1,000 tokens
â”œâ”€â”€ User messages:           ~500 tokens
â”œâ”€â”€ AI responses:            ~2,000 tokens
â”œâ”€â”€ Tool calls:              ~500 tokens
â””â”€â”€ Tool results:            ~150,000 tokens (!!)
    â”œâ”€â”€ Screenshot 1:        ~50,000 tokens (base64)
    â”œâ”€â”€ Screenshot 2:        ~50,000 tokens
    â””â”€â”€ Step details:        ~50,000 tokens

Total: ~154,000 tokens per test run
After 2 runs: 308,000 tokens > 200k limit ðŸ’¥
```

## Design Strategies

### Strategy 1: Artifact Store (Recommended) â­

**Separate storage for large artifacts, references in state.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ARCHITECTURE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Tool Execution                                                â”‚
â”‚        â”‚                                                         â”‚
â”‚        â–¼                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚  Full Result    â”‚                                           â”‚
â”‚   â”‚  + Screenshots  â”‚                                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚            â”‚                                                     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚     â”‚             â”‚                                              â”‚
â”‚     â–¼             â–¼                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚ â”‚ Stream â”‚   â”‚ Artifact   â”‚                                     â”‚
â”‚ â”‚ Full   â”‚   â”‚ Store      â”‚                                     â”‚
â”‚ â”‚ Data   â”‚   â”‚ (Supabase) â”‚                                     â”‚
â”‚ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚     â”‚              â”‚                                             â”‚
â”‚     â”‚              â–¼                                             â”‚
â”‚     â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚     â”‚        â”‚ Reference â”‚                                      â”‚
â”‚     â”‚        â”‚ Only      â”‚                                      â”‚
â”‚     â”‚        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚     â”‚              â”‚                                             â”‚
â”‚     â–¼              â–¼                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚ â”‚Frontendâ”‚   â”‚ LangGraph  â”‚                                     â”‚
â”‚ â”‚ (full) â”‚   â”‚ State      â”‚                                     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ (light)    â”‚                                     â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**

```python
# In tool_executor_node
from src.orchestrator.artifact_store import get_artifact_store

async def tool_executor_node(state: ChatState, config) -> dict:
    artifact_store = get_artifact_store()

    # Execute tool, get full result
    full_result = await execute_tool(tool_call)

    # Store artifacts, get lightweight result
    lightweight_result = artifact_store.store_test_result(full_result)

    # Stream full result to frontend (before storing)
    await stream_to_frontend(full_result)

    # Store only lightweight in state
    return {"messages": [ToolMessage(content=json.dumps(lightweight_result))]}
```

**Pros:**
- Full data preserved for frontend/debugging
- State stays small and fast
- Claude gets summary + can request details
- Works with existing LangGraph checkpointer

**Cons:**
- Additional storage infrastructure
- Need to manage artifact lifecycle

---

### Strategy 2: Hierarchical Memory

**Three-tier memory system:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEMORY HIERARCHY                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  TIER 1: Working Memory (in state)                              â”‚
â”‚  â”œâ”€â”€ Last 5 messages (full detail)                              â”‚
â”‚  â”œâ”€â”€ Current test context                                       â”‚
â”‚  â””â”€â”€ Active tool calls                                          â”‚
â”‚                                                                  â”‚
â”‚  TIER 2: Session Memory (PostgresStore)                         â”‚
â”‚  â”œâ”€â”€ Summarized older messages                                  â”‚
â”‚  â”œâ”€â”€ Test results with artifact refs                            â”‚
â”‚  â””â”€â”€ Learned patterns from this session                         â”‚
â”‚                                                                  â”‚
â”‚  TIER 3: Long-term Memory (Vector Store)                        â”‚
â”‚  â”œâ”€â”€ Similar past failures (semantic search)                    â”‚
â”‚  â”œâ”€â”€ Successful healing strategies                              â”‚
â”‚  â””â”€â”€ Application-specific knowledge                             â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**

```python
class HierarchicalMemory:
    def __init__(self):
        self.working_memory = []  # Recent messages
        self.session_store = PostgresStore()  # Session summaries
        self.vector_store = PgVector()  # Semantic search

    def add_message(self, message: BaseMessage):
        self.working_memory.append(message)

        # If working memory exceeds limit, summarize and move to session
        if len(self.working_memory) > 10:
            summary = self._summarize(self.working_memory[:5])
            self.session_store.put(summary)
            self.working_memory = self.working_memory[5:]

    def get_context(self, query: str) -> List[BaseMessage]:
        """Get relevant context for current query."""
        context = []

        # Always include working memory
        context.extend(self.working_memory)

        # Add relevant session summaries
        relevant = self.session_store.search(query, k=3)
        context.extend(relevant)

        # Add similar past experiences
        similar = self.vector_store.similarity_search(query, k=2)
        context.extend(similar)

        return context
```

---

### Strategy 3: Conversation Compaction

**Periodically summarize and compact conversation:**

```python
async def compact_conversation_node(state: ChatState, config) -> dict:
    """Periodically compact old messages into summaries."""
    messages = state["messages"]

    # If under threshold, no compaction needed
    estimated_tokens = sum(estimate_tokens(m) for m in messages)
    if estimated_tokens < 100000:
        return {"messages": messages}

    # Keep recent messages
    recent = messages[-10:]
    old = messages[:-10]

    # Summarize old messages using a smaller model
    summary = await summarize_with_haiku(old)

    # Create summary message
    summary_message = SystemMessage(
        content=f"[Previous conversation summary]\n{summary}"
    )

    return {"messages": [summary_message] + recent}
```

---

### Strategy 4: Tool-Specific State (Best for Testing)

**Separate state fields for different data types:**

```python
class TestingState(TypedDict):
    # Conversation (kept small)
    messages: Annotated[List[BaseMessage], add_messages]

    # Test execution (separate from messages)
    current_test: Optional[dict]  # Current test being executed
    test_history: List[dict]  # Summaries of past tests

    # Artifacts (references only)
    screenshot_refs: List[str]  # IDs to artifact store

    # Learning (semantic memory)
    failure_patterns: List[dict]  # Known failure patterns
    healing_strategies: List[dict]  # Successful fixes

    # Context (application-specific)
    app_url: str
    discovered_elements: List[dict]  # Page elements cache
```

**Benefits:**
- Messages stay conversational and small
- Test data organized separately
- Easy to query/filter specific data types
- LangGraph can checkpoint each field independently

---

## Recommended Architecture for Argus

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RECOMMENDED DESIGN                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                  LangGraph State                         â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚  â”‚  messages   â”‚  â”‚ test_state  â”‚  â”‚  artifacts  â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  (conv.)    â”‚  â”‚ (current)   â”‚  â”‚  (refs)     â”‚     â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚          â–¼              â–¼              â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ PostgreSQL  â”‚  â”‚  Supabase   â”‚  â”‚  pgvector   â”‚             â”‚
â”‚  â”‚ Checkpointerâ”‚  â”‚  Storage    â”‚  â”‚  Memory     â”‚             â”‚
â”‚  â”‚ (state)     â”‚  â”‚ (artifacts) â”‚  â”‚ (patterns)  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flow:
1. Tool executes â†’ Full result with screenshots
2. Stream full result to frontend immediately
3. Extract artifacts â†’ Store in Supabase Storage
4. Create lightweight result with refs
5. Store lightweight in LangGraph state
6. Prune old messages if needed
7. Claude sees summaries + can request artifact details
```

## Implementation Priority

1. **Phase 1 (Current)**: Message pruning + base64 stripping
   - Quick fix, already implemented
   - Handles immediate overflow issue

2. **Phase 2**: Artifact Store
   - Implement `artifact_store.py`
   - Store screenshots in Supabase Storage
   - Pass refs instead of base64 in state

3. **Phase 3**: Hierarchical Memory
   - Add session summaries
   - Implement vector search for similar failures
   - Enable cross-session learning

4. **Phase 4**: Tool-Specific State
   - Refactor state to separate concerns
   - Add test history tracking
   - Implement element caching

## Code Changes Required

### chat_graph.py
```python
# Add artifact store integration
from src.orchestrator.artifact_store import get_artifact_store

async def tool_executor_node(state: ChatState, config) -> dict:
    artifact_store = get_artifact_store()

    for tool_call in last_message.tool_calls:
        # Execute tool
        result = await execute_tool(tool_call)

        # Extract and store artifacts
        lightweight_result = artifact_store.store_test_result(result)

        # Create tool message with lightweight result
        tool_results.append(ToolMessage(
            content=json.dumps(lightweight_result),
            tool_call_id=tool_call["id"]
        ))

    return {"messages": tool_results}
```

### chat.py (streaming)
```python
# Stream full artifacts before storing lightweight
async def generate_ai_sdk_stream():
    artifact_store = get_artifact_store()

    async for event in app.astream(...):
        if event_type == "values":
            # Get the artifact refs from lightweight result
            result = last_msg.content
            artifact_refs = result.get("_artifact_refs", [])

            # Stream full artifacts to frontend
            for ref in artifact_refs:
                full_artifact = artifact_store.get(ref["artifact_id"])
                yield f'b:{json.dumps({"type": "artifact", "data": full_artifact})}\n'
```

## Summary

| Strategy | Complexity | Effectiveness | Recommended For |
|----------|------------|---------------|-----------------|
| Message Pruning | Low | Medium | Quick fix |
| Artifact Store | Medium | High | Production |
| Hierarchical Memory | High | Very High | Enterprise |
| Tool-Specific State | Medium | High | Complex agents |

**For Argus, implement in this order:**
1. âœ… Message pruning (done)
2. ðŸ”œ Artifact store (next)
3. ðŸ“… Hierarchical memory (future)
4. ðŸ“… Tool-specific state (future)
