# E2E Testing Agent Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# REQUIRED
# =============================================================================

# Anthropic API Key (get from https://console.anthropic.com)
ANTHROPIC_API_KEY=sk-ant-...

# =============================================================================
# OPTIONAL - Integrations
# =============================================================================

# GitHub token for PR integration
GITHUB_TOKEN=ghp_...

# Database connection (for DB testing)
DATABASE_URL=postgresql://user:pass@localhost:5432/dbname

# Slack webhook for notifications
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...

# =============================================================================
# MULTI-MODEL CONFIGURATION (Cost Optimization)
# =============================================================================

# Model routing strategy:
# - anthropic_only: Use only Claude models (simplest, most expensive)
# - cost_optimized: Use cheapest model for each task (60-80% savings)
# - balanced: Balance cost and quality (recommended)
# - quality_first: Use best model, fallback to cheaper
MODEL_STRATEGY=balanced

# Inference Gateway (how to access AI models):
# - cloudflare: Cloudflare AI Gateway (recommended - unified billing, edge caching)
# - direct: Direct API calls to each provider
# - aws_bedrock: AWS Bedrock (enterprise)
# - azure: Azure OpenAI
INFERENCE_GATEWAY=cloudflare

# Cloudflare AI Gateway (if using cloudflare gateway)
CLOUDFLARE_ACCOUNT_ID=your_account_id
CLOUDFLARE_GATEWAY_ID=your_gateway_id

# =============================================================================
# API KEYS - Multi-Provider (for cost optimization)
# =============================================================================

# OpenAI API Key (for GPT-4o, GPT-4o-mini)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# Google AI API Key (for Gemini, Gemini Computer Use)
# Get from: https://aistudio.google.com/apikey
GOOGLE_API_KEY=AIza...

# Groq API Key (for ultra-fast Llama inference - 100ms latency!)
# Get from: https://console.groq.com/keys
GROQ_API_KEY=gsk_...

# Together API Key (for DeepSeek, open models)
# Get from: https://api.together.xyz/settings/api-keys
TOGETHER_API_KEY=...

# =============================================================================
# VERTEX AI CONFIGURATION (Claude via Google Cloud)
# =============================================================================
# Benefits:
# - Unified GCP billing (use committed spend discounts)
# - Enterprise features (VPC-SC, IAM, audit logging)
# - Regional data residency (EU/US compliance)
# - Full Computer Use support
# - Same pricing as direct Anthropic API

# Enable Vertex AI for Claude models (set to true to use Vertex AI)
USE_VERTEX_AI=false

# GCP Project ID (required if USE_VERTEX_AI=true)
# Find in: https://console.cloud.google.com/
GOOGLE_CLOUD_PROJECT=your-gcp-project-id

# Vertex AI Region:
# - "global": Dynamic routing for max availability (recommended, no premium)
# - "us-east1", "europe-west1", etc.: Regional endpoints (10% premium, data residency)
VERTEX_AI_REGION=global

# Setup steps:
# 1. pip install anthropic[vertex]
# 2. gcloud auth application-default login
# 3. Enable Vertex AI API in GCP Console
# 4. Set GOOGLE_CLOUD_PROJECT above

# =============================================================================
# LEGACY MODEL CONFIGURATION (for anthropic_only mode)
# =============================================================================

# Default model for testing (claude-sonnet-4-5 recommended)
DEFAULT_MODEL=claude-sonnet-4-5

# Fast model for quick verifications
VERIFICATION_MODEL=claude-haiku-4-5

# Powerful model for complex debugging
DEBUGGING_MODEL=claude-opus-4-5

# =============================================================================
# ARGUS ARCHITECTURE (Brain + Browser Worker)
# =============================================================================
# Argus has two main components:
# 1. Brain (Python/LangGraph) - Orchestration, code analysis, planning
# 2. Browser Worker (Cloudflare) - Browser automation, self-healing
#
# For local development:
# - Brain: python -m uvicorn src.api.server:app --port 8000
# - Worker: Already deployed at argus-api.samuelvinay-kumar.workers.dev
#
# For production, deploy Brain to Railway/Render/Fly.io

# =============================================================================
# BROWSER AUTOMATION WORKER (Cloudflare - Required)
# =============================================================================

# URL of the Argus Browser Automation Worker
# Default public worker: https://argus-api.samuelvinay-kumar.workers.dev
# To deploy your own: cd cloudflare-worker && npm run deploy
BROWSER_WORKER_URL=https://argus-api.samuelvinay-kumar.workers.dev

# =============================================================================
# DASHBOARD CONFIGURATION
# =============================================================================

# Argus Brain URL (Python backend for full orchestration)
# Set this when deploying the Python backend to Railway/Render/Fly.io
# Local: http://localhost:8000
# Production: https://your-argus-brain.railway.app
ARGUS_BACKEND_URL=http://localhost:8000

# Cloudflare Worker URL for dashboard (browser automation)
E2E_WORKER_URL=https://argus-api.samuelvinay-kumar.workers.dev

# =============================================================================
# LEGACY: STAGEHAND CONFIGURATION (kept for backwards compatibility)
# =============================================================================

# Enable Stagehand for browser automation (now via Cloudflare Worker)
USE_STAGEHAND=true

# Stagehand model provider: "workers-ai" (free), "openai", "anthropic"
STAGEHAND_MODEL_PROVIDER=openai

# Enable selector caching (faster repeated actions)
STAGEHAND_CACHE_ENABLED=true

# Enable self-healing (auto-fix broken selectors)
STAGEHAND_SELF_HEALING=true

# =============================================================================
# LEGACY COMPUTER USE SETTINGS (if not using Stagehand)
# =============================================================================

# Screenshot resolution
SCREENSHOT_WIDTH=1920
SCREENSHOT_HEIGHT=1080

# Maximum iterations per test
MAX_ITERATIONS=50

# Action timeout in milliseconds
ACTION_TIMEOUT_MS=10000

# =============================================================================
# COST CONTROLS
# =============================================================================

# Maximum cost per test run in USD
COST_LIMIT_PER_RUN=10.00

# Maximum cost per individual test in USD
COST_LIMIT_PER_TEST=1.00

# =============================================================================
# EXECUTION SETTINGS
# =============================================================================

# Number of tests to run in parallel
PARALLEL_TESTS=1

# Number of retries for failed tests
RETRY_FAILED_TESTS=2

# Enable automatic test healing
SELF_HEAL_ENABLED=true

# Minimum confidence for auto-healing (0.0 to 1.0)
SELF_HEAL_CONFIDENCE_THRESHOLD=0.8

# =============================================================================
# OUTPUT SETTINGS
# =============================================================================

# Directory for test outputs
OUTPUT_DIR=./test-results

# Directory for screenshots
SCREENSHOT_DIR=./test-results/screenshots

# =============================================================================
# SERVER SETTINGS (for webhook mode)
# =============================================================================

# Server host
SERVER_HOST=0.0.0.0

# Server port
SERVER_PORT=8000
