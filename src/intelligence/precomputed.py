"""Precomputed Intelligence Results Reader.

Reads precomputed results from Supabase for instant lookups.
Falls back gracefully when results are expired or missing.
"""

from dataclasses import dataclass
from datetime import UTC, datetime, timedelta
from typing import Any, Optional

import structlog

from src.services.supabase_client import get_supabase_client

logger = structlog.get_logger()


@dataclass
class PrecomputedResult:
    """A precomputed intelligence result with freshness metadata."""

    data: dict[str, Any]
    computation_type: str
    computed_at: datetime
    valid_until: datetime
    is_fresh: bool  # True if computed within last hour

    @classmethod
    def from_db_row(cls, row: dict[str, Any]) -> "PrecomputedResult":
        """Create a PrecomputedResult from a database row."""
        computed_at = cls._parse_timestamp(row.get("computed_at"))
        valid_until = cls._parse_timestamp(row.get("valid_until"))
        now = datetime.now(UTC)

        # Consider fresh if computed within the last hour
        is_fresh = (now - computed_at) < timedelta(hours=1) if computed_at else False

        return cls(
            data=row.get("result", {}),
            computation_type=row.get("computation_type", ""),
            computed_at=computed_at or now,
            valid_until=valid_until or now,
            is_fresh=is_fresh,
        )

    @staticmethod
    def _parse_timestamp(value: Any) -> Optional[datetime]:
        """Parse a timestamp value from the database."""
        if value is None:
            return None
        if isinstance(value, datetime):
            return value if value.tzinfo else value.replace(tzinfo=UTC)
        if isinstance(value, str):
            # Handle ISO format with or without timezone
            try:
                dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
                return dt if dt.tzinfo else dt.replace(tzinfo=UTC)
            except ValueError:
                return None
        return None


class PrecomputedReader:
    """Read precomputed intelligence results from Supabase.

    This class provides methods to retrieve various precomputed intelligence
    results that are generated by background jobs (e.g., Flink streaming).
    Results are cached in the intelligence_precomputed table with TTL.

    Available computation types:
        - test_impact_matrix: file -> tests mapping for selective test execution
        - failure_clusters: grouped failure patterns for root cause analysis
        - coverage_gaps: uncovered code paths needing tests
        - flaky_ranking: tests ranked by unreliability score
    """

    def __init__(self):
        """Initialize the PrecomputedReader."""
        self.client = get_supabase_client()
        self.log = structlog.get_logger().bind(component="precomputed")

    async def get_test_impact_matrix(
        self, org_id: str, project_id: str
    ) -> Optional[dict[str, list[str]]]:
        """Get file -> tests mapping for selective test execution.

        Args:
            org_id: Organization ID for tenant isolation
            project_id: Project ID

        Returns:
            Dictionary mapping file paths to list of test IDs that cover them,
            or None if no precomputed result is available.

        Example:
            {
                "src/api/users.py": ["test_user_create", "test_user_update"],
                "src/models/user.py": ["test_user_create", "test_user_model"]
            }
        """
        result = await self._get_precomputed(org_id, project_id, "test_impact_matrix")
        if result is None:
            return None

        self.log.debug(
            "Retrieved test impact matrix",
            org_id=org_id,
            project_id=project_id,
            is_fresh=result.is_fresh,
            file_count=len(result.data) if isinstance(result.data, dict) else 0,
        )

        # Validate and return the mapping
        if isinstance(result.data, dict):
            return result.data
        return None

    async def get_failure_clusters(
        self, org_id: str, project_id: str
    ) -> Optional[list[dict]]:
        """Get clustered failure patterns for root cause analysis.

        Args:
            org_id: Organization ID for tenant isolation
            project_id: Project ID

        Returns:
            List of failure cluster dictionaries, or None if unavailable.

        Example:
            [
                {
                    "cluster_id": "auth-timeout-001",
                    "pattern": "Connection timeout in auth service",
                    "error_count": 42,
                    "affected_tests": ["test_login", "test_oauth"],
                    "root_cause": "Rate limiting on auth provider",
                    "first_seen": "2024-01-15T10:30:00Z",
                    "last_seen": "2024-01-20T14:22:00Z"
                }
            ]
        """
        result = await self._get_precomputed(org_id, project_id, "failure_clusters")
        if result is None:
            return None

        self.log.debug(
            "Retrieved failure clusters",
            org_id=org_id,
            project_id=project_id,
            is_fresh=result.is_fresh,
            cluster_count=len(result.data) if isinstance(result.data, list) else 0,
        )

        # The data should be a list of cluster objects
        if isinstance(result.data, list):
            return result.data
        # Handle case where data is wrapped in a container
        if isinstance(result.data, dict) and "clusters" in result.data:
            return result.data["clusters"]
        return None

    async def get_coverage_gaps(
        self, org_id: str, project_id: str
    ) -> Optional[list[dict]]:
        """Get uncovered code paths needing tests.

        Args:
            org_id: Organization ID for tenant isolation
            project_id: Project ID

        Returns:
            List of coverage gap dictionaries, or None if unavailable.

        Example:
            [
                {
                    "file_path": "src/api/payments.py",
                    "uncovered_lines": [45, 46, 47, 78, 79],
                    "function_name": "process_refund",
                    "coverage_percent": 23.5,
                    "priority": "high",
                    "suggested_test": "test_refund_flow"
                }
            ]
        """
        result = await self._get_precomputed(org_id, project_id, "coverage_gaps")
        if result is None:
            return None

        self.log.debug(
            "Retrieved coverage gaps",
            org_id=org_id,
            project_id=project_id,
            is_fresh=result.is_fresh,
            gap_count=len(result.data) if isinstance(result.data, list) else 0,
        )

        if isinstance(result.data, list):
            return result.data
        if isinstance(result.data, dict) and "gaps" in result.data:
            return result.data["gaps"]
        return None

    async def get_flaky_ranking(
        self, org_id: str, project_id: str
    ) -> Optional[list[dict]]:
        """Get flaky tests ranked by unreliability score.

        Args:
            org_id: Organization ID for tenant isolation
            project_id: Project ID

        Returns:
            List of flaky test dictionaries sorted by unreliability,
            or None if unavailable.

        Example:
            [
                {
                    "test_id": "test_async_upload",
                    "test_name": "Test async file upload",
                    "flakiness_score": 0.85,
                    "total_runs": 100,
                    "failures": 15,
                    "failure_rate": 0.15,
                    "common_errors": ["Timeout waiting for element"],
                    "last_failure": "2024-01-20T09:15:00Z",
                    "recommendation": "Add explicit wait for upload completion"
                }
            ]
        """
        result = await self._get_precomputed(org_id, project_id, "flaky_ranking")
        if result is None:
            return None

        self.log.debug(
            "Retrieved flaky ranking",
            org_id=org_id,
            project_id=project_id,
            is_fresh=result.is_fresh,
            test_count=len(result.data) if isinstance(result.data, list) else 0,
        )

        if isinstance(result.data, list):
            return result.data
        if isinstance(result.data, dict) and "ranking" in result.data:
            return result.data["ranking"]
        return None

    async def _get_precomputed(
        self, org_id: str, project_id: str, computation_type: str
    ) -> Optional[PrecomputedResult]:
        """Generic precomputed result getter.

        Fetches a precomputed result from the database, checking validity.

        Args:
            org_id: Organization ID for tenant isolation
            project_id: Project ID (UUID string)
            computation_type: Type of computation to retrieve

        Returns:
            PrecomputedResult if found and valid, None otherwise.
        """
        if not self.client.is_configured:
            self.log.warning("Supabase not configured, cannot fetch precomputed results")
            return None

        try:
            # Try using the RPC function first (more efficient)
            rpc_result = await self.client.rpc(
                "get_precomputed",
                {
                    "p_org_id": org_id,
                    "p_project_id": project_id,
                    "p_type": computation_type,
                },
            )

            if rpc_result.get("error"):
                # Fall back to direct query if RPC fails
                self.log.debug(
                    "RPC get_precomputed failed, falling back to direct query",
                    error=rpc_result.get("error"),
                )
                return await self._get_precomputed_direct(
                    org_id, project_id, computation_type
                )

            data = rpc_result.get("data")
            if data is None:
                self.log.debug(
                    "No precomputed result found",
                    org_id=org_id,
                    project_id=project_id,
                    computation_type=computation_type,
                )
                return None

            # RPC returns just the JSONB result, need to construct PrecomputedResult
            # with minimal metadata since RPC only returns the result field
            return PrecomputedResult(
                data=data,
                computation_type=computation_type,
                computed_at=datetime.now(UTC),  # Unknown from RPC
                valid_until=datetime.now(UTC) + timedelta(hours=1),  # Assume valid
                is_fresh=True,  # Assume fresh since it passed validity check in DB
            )

        except Exception as e:
            self.log.exception(
                "Error fetching precomputed result",
                org_id=org_id,
                project_id=project_id,
                computation_type=computation_type,
                error=str(e),
            )
            return None

    async def _get_precomputed_direct(
        self, org_id: str, project_id: str, computation_type: str
    ) -> Optional[PrecomputedResult]:
        """Fetch precomputed result via direct table query.

        This is a fallback when the RPC function is not available.

        Args:
            org_id: Organization ID
            project_id: Project ID
            computation_type: Type of computation

        Returns:
            PrecomputedResult if found and valid, None otherwise.
        """
        try:
            # Query the table directly with validity check
            # Note: We use now() check on the server side via the query
            result = await self.client.request(
                f"/intelligence_precomputed"
                f"?org_id=eq.{org_id}"
                f"&project_id=eq.{project_id}"
                f"&computation_type=eq.{computation_type}"
                f"&valid_until=gt.now()"
                f"&select=result,computation_type,computed_at,valid_until"
                f"&limit=1"
            )

            if result.get("error"):
                error_msg = str(result.get("error", ""))
                # Handle missing table gracefully
                if (
                    "does not exist" in error_msg
                    or "42P01" in error_msg
                    or "42703" in error_msg
                ):
                    self.log.warning(
                        "intelligence_precomputed table not found",
                        error=error_msg,
                    )
                    return None
                self.log.error(
                    "Error querying precomputed results",
                    error=error_msg,
                )
                return None

            data = result.get("data")
            if not data or len(data) == 0:
                self.log.debug(
                    "No valid precomputed result found",
                    org_id=org_id,
                    project_id=project_id,
                    computation_type=computation_type,
                )
                return None

            row = data[0]
            return PrecomputedResult.from_db_row(row)

        except Exception as e:
            self.log.exception(
                "Error in direct precomputed query",
                org_id=org_id,
                project_id=project_id,
                computation_type=computation_type,
                error=str(e),
            )
            return None

    async def get_all_for_project(
        self, org_id: str, project_id: str
    ) -> dict[str, PrecomputedResult]:
        """Get all precomputed results for a project.

        Useful for dashboard views that need multiple metrics at once.

        Args:
            org_id: Organization ID
            project_id: Project ID

        Returns:
            Dictionary mapping computation_type to PrecomputedResult.
        """
        if not self.client.is_configured:
            return {}

        try:
            result = await self.client.request(
                f"/intelligence_precomputed"
                f"?org_id=eq.{org_id}"
                f"&project_id=eq.{project_id}"
                f"&valid_until=gt.now()"
                f"&select=result,computation_type,computed_at,valid_until"
            )

            if result.get("error") or not result.get("data"):
                return {}

            results = {}
            for row in result["data"]:
                precomputed = PrecomputedResult.from_db_row(row)
                results[precomputed.computation_type] = precomputed

            self.log.debug(
                "Retrieved all precomputed results",
                org_id=org_id,
                project_id=project_id,
                result_count=len(results),
                types=list(results.keys()),
            )

            return results

        except Exception as e:
            self.log.exception(
                "Error fetching all precomputed results",
                org_id=org_id,
                project_id=project_id,
                error=str(e),
            )
            return {}

    async def is_stale(
        self, org_id: str, project_id: str, computation_type: str
    ) -> bool:
        """Check if a precomputed result is stale or missing.

        Useful for triggering background refresh jobs.

        Args:
            org_id: Organization ID
            project_id: Project ID
            computation_type: Type of computation to check

        Returns:
            True if result is stale, expired, or missing.
        """
        result = await self._get_precomputed(org_id, project_id, computation_type)
        if result is None:
            return True
        return not result.is_fresh


# Module-level singleton for convenience
_reader: Optional[PrecomputedReader] = None


def get_precomputed_reader() -> PrecomputedReader:
    """Get or create the global PrecomputedReader instance."""
    global _reader
    if _reader is None:
        _reader = PrecomputedReader()
    return _reader
