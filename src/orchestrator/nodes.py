"""Node implementations for the LangGraph orchestrator."""

import json
import re
import time
from typing import Any

import anthropic
import structlog

from ..config import MODEL_PRICING, get_settings
from ..security import (
    create_secure_reader,
    get_audit_logger,
    hash_content,
)
from ..services.dependency_analyzer import DependencyAnalyzer, ImpactResult
from ..services.git_analyzer import GitAnalyzer
from ..services.source_analyzer import SourceAnalyzer
from .state import FailureAnalysis, TestingState, TestResult, TestStatus

logger = structlog.get_logger()


def robust_json_parse(content: str) -> dict:
    """Parse JSON from LLM output with error handling for common issues.

    Handles:
    - Trailing commas
    - Single quotes instead of double quotes
    - Unquoted keys
    - Comments
    - Markdown code blocks
    """
    # Extract JSON from markdown code blocks
    if "```json" in content:
        content = content.split("```json")[1].split("```")[0]
    elif "```" in content:
        content = content.split("```")[1].split("```")[0]

    content = content.strip()

    # Try direct parse first
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Remove trailing commas before ] or }
    content = re.sub(r',\s*([}\]])', r'\1', content)

    # Remove JavaScript-style comments
    content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
    content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)

    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # Try to find the first { and last } to extract just the JSON object
    start = content.find('{')
    end = content.rfind('}')
    if start != -1 and end != -1 and end > start:
        try:
            return json.loads(content[start:end+1])
        except json.JSONDecodeError:
            pass

    # Try to find array
    start = content.find('[')
    end = content.rfind(']')
    if start != -1 and end != -1 and end > start:
        try:
            return json.loads(content[start:end+1])
        except json.JSONDecodeError:
            pass

    # Return empty dict as fallback
    logger.warning("Failed to parse JSON from LLM output", content_preview=content[:200])
    return {}


def _track_usage(state: TestingState, response: Any) -> TestingState:
    """Track API usage and costs."""
    if hasattr(response, "usage"):
        input_tokens = response.usage.input_tokens
        output_tokens = response.usage.output_tokens

        settings = get_settings()
        pricing = MODEL_PRICING[settings.default_model]
        cost = (
            input_tokens * pricing["input"] / 1_000_000 +
            output_tokens * pricing["output"] / 1_000_000
        )

        state["total_input_tokens"] += input_tokens
        state["total_output_tokens"] += output_tokens
        state["total_cost"] += cost

    state["iteration"] += 1
    return state


async def _emit_test_events(
    state: TestingState,
    test: dict,
    test_result: "TestResult",
    log,
) -> None:
    """Emit test execution events to Redpanda for downstream processing.

    Emits:
    - TEST_EXECUTED for all test completions (pass/fail)
    - TEST_FAILED for failures (triggers self-healing pipeline)
    """
    # Skip event emission if no org_id is configured
    org_id = state.get("org_id")
    if not org_id:
        log.debug("Skipping event emission - no org_id configured")
        return

    try:
        from ..services.event_gateway import (
            EventType,
            emit_test_executed,
            emit_test_failed,
            get_event_gateway,
        )

        event_gateway = get_event_gateway()
        if not event_gateway.is_running:
            log.debug("Event gateway not running, skipping event emission")
            return

        project_id = state.get("project_id")
        user_id = state.get("user_id")
        run_id = state.get("run_id")

        # Always emit TEST_EXECUTED event for all test completions
        await emit_test_executed(
            test_id=test["id"],
            test_name=test.get("name", "Unknown"),
            status=test_result.status.value,
            duration_ms=int(test_result.duration_seconds * 1000),
            org_id=org_id,
            project_id=project_id,
            user_id=user_id,
            correlation_id=run_id,
            steps_count=len(test.get("steps", [])),
            assertions_count=test_result.assertions_passed + test_result.assertions_failed,
            metadata={
                "run_id": run_id,
                "test_type": test.get("type", "unknown"),
                "priority": test.get("priority", "medium"),
                "assertions_passed": test_result.assertions_passed,
                "assertions_failed": test_result.assertions_failed,
            },
        )
        log.debug("Emitted TEST_EXECUTED event", test_id=test["id"], status=test_result.status.value)

        # Emit TEST_FAILED event for failures (triggers self-healing pipeline)
        if test_result.status == TestStatus.FAILED:
            # Determine failure type based on error message
            error_msg = test_result.error_message or "Unknown error"
            failure_type = _classify_failure_type(error_msg)

            await emit_test_failed(
                test_id=test["id"],
                test_name=test.get("name", "Unknown"),
                error_message=error_msg,
                failure_type=failure_type,
                org_id=org_id,
                project_id=project_id,
                user_id=user_id,
                correlation_id=run_id,
                stack_trace=None,  # Could be extracted from error if available
                screenshot_url=None,  # Screenshots are base64, would need upload
                metadata={
                    "run_id": run_id,
                    "test_type": test.get("type", "unknown"),
                    "failed_step": _get_failed_step(test_result.actions_taken),
                    "actions_taken": len(test_result.actions_taken),
                },
            )
            log.debug("Emitted TEST_FAILED event", test_id=test["id"], failure_type=failure_type)

    except ImportError as e:
        log.warning("Event gateway module not available", error=str(e))
    except Exception as e:
        # Event emission is non-critical - log but don't fail the test execution
        log.warning("Failed to emit test events", error=str(e), test_id=test["id"])


def _classify_failure_type(error_message: str) -> str:
    """Classify failure type based on error message patterns."""
    error_lower = error_message.lower()

    if any(kw in error_lower for kw in ["timeout", "timed out", "deadline"]):
        return "timeout"
    elif any(kw in error_lower for kw in ["selector", "element", "not found", "locator"]):
        return "selector"
    elif any(kw in error_lower for kw in ["assert", "expected", "mismatch"]):
        return "assertion"
    elif any(kw in error_lower for kw in ["network", "connection", "refused", "dns"]):
        return "network"
    elif any(kw in error_lower for kw in ["script", "javascript", "syntax"]):
        return "script"
    else:
        return "unknown"


def _get_failed_step(actions_taken: list[dict]) -> int | None:
    """Get the step number that failed from actions taken."""
    for action in actions_taken:
        if action.get("result") == "failure":
            return action.get("step")
    return None


async def analyze_code_node(state: TestingState) -> TestingState:
    """
    Analyze the codebase to understand structure and identify testable surfaces.

    This node:
    1. SECURELY reads the codebase (sanitizes secrets, respects consent)
    2. Scans for routes, components, API endpoints
    3. Identifies authentication flows
    4. Maps database models
    5. Creates a summary for test planning

    Security features:
    - Secrets are automatically redacted before sending to AI
    - Restricted files (.env, credentials, keys) are skipped
    - All file access is logged for audit
    - User consent is verified before processing
    """
    log = logger.bind(node="analyze_code")
    log.info("Analyzing codebase", path=state["codebase_path"])

    settings = get_settings()
    audit = get_audit_logger()

    # Create secure code reader with auto-consent in standard mode
    # In production, you'd prompt for consent if not granted
    user_id = state.get("user_id", "anonymous")
    session_id = state.get("session_id")

    try:
        # 1. SECURELY read the codebase
        reader = create_secure_reader(
            user_id=user_id,
            session_id=session_id,
            auto_consent_mode="standard",  # Auto-grants standard consents for CLI usage
        )

        log.info("Reading codebase with security sanitization...")

        # Read and sanitize all code files
        # Reduced limits to stay within Claude's 200K token context window
        # ~500KB of code â‰ˆ ~125K tokens (4 chars per token)
        read_results = reader.read_codebase(
            state["codebase_path"],
            max_files=50,  # Reduced from 150 for context window
            max_total_size_kb=500,  # Reduced from 1500KB to ~500KB
        )

        # Get file summary for logging
        file_summary = reader.get_file_summary(read_results)
        log.info(
            "Codebase read complete",
            files_read=file_summary["readable"],
            files_skipped=file_summary["skipped"],
            secrets_redacted=file_summary["secrets_redacted"],
        )

        # Format sanitized code for AI consumption
        code_context = reader.get_context_for_ai(read_results)

        # If we have changed files, prioritize reading those
        changed_files_context = ""
        if state.get("changed_files"):
            changed_results = []
            for cf in state["changed_files"]:
                from pathlib import Path
                cf_path = Path(state["codebase_path"]) / cf
                if cf_path.exists():
                    result = reader.read_file(cf_path)
                    if not result.skipped:
                        changed_results.append(result)

            if changed_results:
                changed_files_context = "\n\n# CHANGED FILES (Priority)\n"
                changed_files_context += reader.get_context_for_ai(changed_results, include_metadata=False)

    except PermissionError as e:
        log.error("Consent not granted", error=str(e))
        state["error"] = f"Consent required: {str(e)}"
        return state
    except Exception as e:
        log.error("Failed to read codebase", error=str(e))
        state["error"] = f"Codebase reading failed: {str(e)}"
        return state

    # 2. Run analysis services for enhanced code understanding
    dependency_analysis = {}
    git_analysis = {}
    source_analysis = {}
    impact_analysis = None

    try:
        # 2a. Build dependency graph for test impact analysis
        log.info("Building dependency graph...")
        dep_analyzer = DependencyAnalyzer(state["codebase_path"])
        dep_analyzer.build_graph()

        dependency_analysis = {
            "total_modules": len(dep_analyzer.modules),
            "total_components": len(dep_analyzer.components),
            "total_routes": len(dep_analyzer.routes),
            "untested_components": dep_analyzer.get_untested_components()[:10],  # Top 10
            "dependency_graph_summary": {
                "modules_by_type": {},
                "key_components": [],
            },
        }

        # Summarize modules by type
        for path, module in list(dep_analyzer.modules.items())[:100]:
            mod_type = module.module_type
            if mod_type not in dependency_analysis["dependency_graph_summary"]["modules_by_type"]:
                dependency_analysis["dependency_graph_summary"]["modules_by_type"][mod_type] = 0
            dependency_analysis["dependency_graph_summary"]["modules_by_type"][mod_type] += 1

        # Find key components (most dependents)
        components_with_dependents = [
            (name, len(dep_analyzer.modules.get(comp.file_path, DependencyAnalyzer).dependents if dep_analyzer.modules.get(comp.file_path) else []))
            for name, comp in list(dep_analyzer.components.items())[:50]
        ]
        components_with_dependents.sort(key=lambda x: x[1], reverse=True)
        dependency_analysis["dependency_graph_summary"]["key_components"] = [
            {"name": name, "dependents": count}
            for name, count in components_with_dependents[:10]
        ]

        # 2b. If we have changed files, do impact analysis
        if state.get("changed_files"):
            impact_analysis = dep_analyzer.analyze_impact(state["changed_files"])
            dependency_analysis["impact_analysis"] = {
                "changed_files": impact_analysis.changed_files,
                "affected_modules": impact_analysis.affected_modules[:20],
                "affected_components": impact_analysis.affected_components[:20],
                "affected_routes": impact_analysis.affected_routes[:20],
                "affected_tests": impact_analysis.affected_tests,
                "skipped_tests": impact_analysis.skipped_tests[:20],
                "confidence": impact_analysis.confidence,
                "explanation": impact_analysis.explanation,
            }
            log.info(
                "Impact analysis complete",
                affected_tests=len(impact_analysis.affected_tests),
                skipped_tests=len(impact_analysis.skipped_tests),
            )

        log.info(
            "Dependency analysis complete",
            modules=dependency_analysis["total_modules"],
            components=dependency_analysis["total_components"],
            routes=dependency_analysis["total_routes"],
        )

    except Exception as e:
        log.warning("Dependency analysis failed (non-fatal)", error=str(e))
        dependency_analysis = {"error": str(e)}

    try:
        # 2c. Git analysis for change history and selector tracking
        log.info("Analyzing git history...")
        git_analyzer = GitAnalyzer(state["codebase_path"])

        # Get recent commits
        recent_commits = await git_analyzer.get_recent_commits(days=7, max_commits=20)

        git_analysis = {
            "recent_commits_count": len(recent_commits),
            "recent_commits": [
                {
                    "sha": c.short_sha,
                    "author": c.author,
                    "message": c.message[:100],
                    "files_changed": c.files_changed[:5],
                }
                for c in recent_commits[:10]
            ],
            "contributors": list(set(c.author for c in recent_commits)),
            "files_changed_recently": list(set(
                f for c in recent_commits for f in c.files_changed
            ))[:30],
        }

        # If we have changed files, find selector changes
        if state.get("changed_files"):
            selector_changes = []
            for cf in state["changed_files"][:5]:  # Limit to first 5 files
                component_history = await git_analyzer.get_component_history(cf, days=14)
                for change in component_history[:3]:
                    if change.selectors_affected:
                        selector_changes.extend(change.selectors_affected[:5])

            if selector_changes:
                git_analysis["recent_selector_changes"] = list(set(selector_changes))[:20]

        log.info(
            "Git analysis complete",
            commits=git_analysis["recent_commits_count"],
            contributors=len(git_analysis["contributors"]),
        )

    except Exception as e:
        log.warning("Git analysis failed (non-fatal)", error=str(e))
        git_analysis = {"error": str(e)}

    try:
        # 2d. Source analysis for selector extraction and stability scoring
        log.info("Analyzing source code for selectors...")
        source_analyzer = SourceAnalyzer(state["codebase_path"])

        # Analyze source directories
        components_info = source_analyzer.analyze_directory("src")
        if not components_info:
            # Try common alternative directories
            for alt_dir in ["app", "pages", "components", "lib"]:
                components_info.update(source_analyzer.analyze_directory(alt_dir))

        # Get all selectors with stability info
        all_selectors = source_analyzer.get_all_selectors()
        testid_selectors = source_analyzer.find_selectors_by_type("testid")
        aria_selectors = source_analyzer.find_selectors_by_type("aria")

        # Group selectors by stability
        high_stability = [s for s in all_selectors if s.selector_type in ("testid", "id", "aria")]
        low_stability = [s for s in all_selectors if s.selector_type in ("class", "text")]

        source_analysis = {
            "total_components_analyzed": len(components_info),
            "total_selectors_found": len(all_selectors),
            "selector_breakdown": {
                "testid": len(testid_selectors),
                "aria": len(aria_selectors),
                "high_stability": len(high_stability),
                "low_stability": len(low_stability),
            },
            "component_selector_map": source_analyzer.get_component_selector_map(),
            "testable_elements": [
                {
                    "selector": s.selector,
                    "type": s.selector_type,
                    "element": s.element_type,
                    "file": s.file_path,
                    "semantic": s.semantic_name,
                }
                for s in testid_selectors[:30]  # Top 30 testid selectors
            ],
            "selector_recommendations": [],
        }

        # Generate recommendations for low-stability selectors in key components
        for selector in low_stability[:10]:
            mapping = source_analyzer.get_selector_mapping(selector.selector)
            if mapping and mapping.recommendation:
                source_analysis["selector_recommendations"].append({
                    "selector": selector.selector,
                    "file": selector.file_path,
                    "recommendation": mapping.recommendation,
                    "stability_score": mapping.stability_score,
                })

        log.info(
            "Source analysis complete",
            components=source_analysis["total_components_analyzed"],
            selectors=source_analysis["total_selectors_found"],
            testids=source_analysis["selector_breakdown"]["testid"],
        )

    except Exception as e:
        log.warning("Source analysis failed (non-fatal)", error=str(e))
        source_analysis = {"error": str(e)}

    # 2e. Store analysis results in Cognee knowledge graph for future semantic search
    try:
        from ..knowledge import get_cognee_client

        org_id = state.get("org_id")
        project_id = state.get("project_id")

        if org_id and project_id:
            cognee_client = get_cognee_client(org_id=org_id, project_id=project_id)

            # Store codebase structure in knowledge graph
            codebase_knowledge = {
                "type": "codebase_analysis",
                "path": state["codebase_path"],
                "app_url": state["app_url"],
                "dependency_analysis": dependency_analysis,
                "git_analysis": git_analysis,
                "source_analysis": {
                    k: v for k, v in source_analysis.items()
                    if k != "component_selector_map"  # Too large for embedding
                },
            }

            await cognee_client.add_to_knowledge_graph(
                content=codebase_knowledge,
                content_type="codebase",
            )

            log.info("Stored codebase analysis in Cognee knowledge graph")
    except Exception as e:
        log.warning("Failed to store in Cognee (non-fatal)", error=str(e))

    # 3. Send to Claude for analysis with enhanced context from analysis services
    client = anthropic.Anthropic(api_key=settings.anthropic_api_key.get_secret_value())

    # Build enhanced analysis context from services
    analysis_context = ""

    # Add dependency analysis context
    if dependency_analysis and "error" not in dependency_analysis:
        analysis_context += f"""
## DEPENDENCY ANALYSIS
- Total modules: {dependency_analysis.get('total_modules', 0)}
- Total components: {dependency_analysis.get('total_components', 0)}
- Total routes: {dependency_analysis.get('total_routes', 0)}
- Modules by type: {json.dumps(dependency_analysis.get('dependency_graph_summary', {}).get('modules_by_type', {}), indent=2)}
- Key components (most dependencies): {json.dumps(dependency_analysis.get('dependency_graph_summary', {}).get('key_components', [])[:5], indent=2)}
"""
        if "impact_analysis" in dependency_analysis:
            ia = dependency_analysis["impact_analysis"]
            analysis_context += f"""
### TEST IMPACT ANALYSIS (for changed files)
- Affected components: {len(ia.get('affected_components', []))}
- Affected routes: {ia.get('affected_routes', [])}
- Tests to run: {len(ia.get('affected_tests', []))}
- Tests to skip: {len(ia.get('skipped_tests', []))}
- Explanation: {ia.get('explanation', 'N/A')}
"""

    # Add git analysis context
    if git_analysis and "error" not in git_analysis:
        analysis_context += f"""
## GIT HISTORY ANALYSIS
- Recent commits (7 days): {git_analysis.get('recent_commits_count', 0)}
- Contributors: {', '.join(git_analysis.get('contributors', [])[:5])}
- Recently changed files: {', '.join(git_analysis.get('files_changed_recently', [])[:10])}
"""
        if git_analysis.get("recent_selector_changes"):
            analysis_context += f"""- Selectors changed recently: {', '.join(git_analysis['recent_selector_changes'][:10])}
"""

    # Add source analysis context
    if source_analysis and "error" not in source_analysis:
        analysis_context += f"""
## SOURCE CODE ANALYSIS
- Components analyzed: {source_analysis.get('total_components_analyzed', 0)}
- Total selectors found: {source_analysis.get('total_selectors_found', 0)}
- Selector breakdown:
  - data-testid selectors: {source_analysis.get('selector_breakdown', {}).get('testid', 0)}
  - aria selectors: {source_analysis.get('selector_breakdown', {}).get('aria', 0)}
  - High stability selectors: {source_analysis.get('selector_breakdown', {}).get('high_stability', 0)}
  - Low stability selectors: {source_analysis.get('selector_breakdown', {}).get('low_stability', 0)}
"""
        if source_analysis.get("testable_elements"):
            analysis_context += """
### AVAILABLE TEST SELECTORS (data-testid)
"""
            for elem in source_analysis["testable_elements"][:15]:
                analysis_context += f"- `{elem['selector']}` ({elem.get('element', 'unknown')} in {elem['file']})\n"

        if source_analysis.get("selector_recommendations"):
            analysis_context += """
### SELECTOR STABILITY RECOMMENDATIONS
"""
            for rec in source_analysis["selector_recommendations"][:5]:
                analysis_context += f"- {rec['selector']}: {rec['recommendation']} (stability: {rec['stability_score']:.2f})\n"

    prompt = f"""Analyze this codebase and identify testable surfaces.

{changed_files_context}

{code_context}

{analysis_context}

APP URL: {state["app_url"]}

Based on the code and analysis above, identify:
1. User-facing pages/routes with their URLs
2. API endpoints with methods and expected responses
3. Authentication/authorization flows
4. Critical user journeys (signup, login, checkout, etc.)
5. Database operations that need validation

IMPORTANT:
- Prioritize tests for components with many dependents (high impact)
- Use the available data-testid selectors when specifying test targets
- If impact analysis is available, focus on affected areas first
- Note any selector stability issues for the test plan

Note: Some content has been redacted for security ([REDACTED] markers).
Focus on the structure and flow, not the redacted values.

Respond with JSON:
{{
    "summary": "Brief description of the application",
    "framework": "detected framework (react, vue, express, django, etc.)",
    "testable_surfaces": [
        {{
            "type": "ui|api|db",
            "name": "descriptive name",
            "path": "URL or endpoint path",
            "priority": "critical|high|medium|low",
            "description": "what this does",
            "test_scenarios": ["list of scenarios to test"],
            "selectors": ["preferred selectors to use"],
            "affected_by_changes": true|false
        }}
    ]
}}
"""

    # Log the AI request for audit
    prompt_hash = hash_content(prompt)
    audit.log_ai_request(
        user_id=user_id,
        model=settings.default_model.value,
        action="analyze_code",
        prompt_hash=prompt_hash,
        input_tokens=len(prompt) // 4,  # Rough estimate
        session_id=session_id,
        metadata={
            "files_analyzed": file_summary["readable"],
            "secrets_redacted": file_summary["secrets_redacted"],
        }
    )

    try:
        response = client.messages.create(
            model=settings.default_model.value,
            max_tokens=4096,
            messages=[{"role": "user", "content": prompt}],
        )

        state = _track_usage(state, response)

        # Log AI response for audit
        audit.log_ai_response(
            request_id=prompt_hash,
            user_id=user_id,
            model=settings.default_model.value,
            output_tokens=response.usage.output_tokens if hasattr(response, 'usage') else 0,
            cost_usd=state.get("total_cost", 0),
            success=True,
        )

        # Parse response using robust parser
        content = response.content[0].text
        result = robust_json_parse(content)

        state["codebase_summary"] = result.get("summary", "")
        state["testable_surfaces"] = result.get("testable_surfaces", [])

        # Store security metadata
        state["security_summary"] = {
            "files_analyzed": file_summary["readable"],
            "files_skipped": file_summary["skipped"],
            "secrets_redacted": file_summary["secrets_redacted"],
            "sensitivity_breakdown": file_summary["by_sensitivity"],
        }

        # Store analysis service results for downstream nodes
        state["dependency_analysis"] = dependency_analysis
        state["git_analysis"] = git_analysis
        state["source_analysis"] = source_analysis

        # Store impact analysis separately if available
        if impact_analysis:
            state["impact_analysis"] = {
                "affected_tests": impact_analysis.affected_tests,
                "skipped_tests": impact_analysis.skipped_tests,
                "affected_components": impact_analysis.affected_components,
                "affected_routes": impact_analysis.affected_routes,
                "confidence": impact_analysis.confidence,
                "explanation": impact_analysis.explanation,
            }

        log.info(
            "Analysis complete",
            surfaces_found=len(state["testable_surfaces"]),
            files_analyzed=file_summary["readable"],
            secrets_redacted=file_summary["secrets_redacted"],
            modules_analyzed=dependency_analysis.get("total_modules", 0) if dependency_analysis else 0,
            selectors_found=source_analysis.get("total_selectors_found", 0) if source_analysis else 0,
        )

    except Exception as e:
        log.error("Analysis failed", error=str(e))
        audit.log_ai_response(
            request_id=prompt_hash,
            user_id=user_id,
            model=settings.default_model.value,
            output_tokens=0,
            cost_usd=0,
            success=False,
            error_message=str(e),
        )
        state["error"] = f"Code analysis failed: {str(e)}"

    return state


async def plan_tests_node(state: TestingState) -> TestingState:
    """
    Create a prioritized test plan based on analyzed surfaces.

    This node uses the TestPlannerAgent to:
    1. Generate test specs for each testable surface
    2. Prioritize based on criticality and changed files
    3. Create a complete test plan with session configuration

    The TestPlannerAgent provides intelligent test planning with:
    - Enhanced system prompts for better test generation
    - Session configuration estimation for resource allocation
    - Support for API-specific test generation
    """
    log = logger.bind(node="plan_tests")
    log.info("Creating test plan using TestPlannerAgent")

    try:
        from ..agents.test_planner import TestPlannerAgent

        agent = TestPlannerAgent()
        result = await agent.execute(
            testable_surfaces=state.get("testable_surfaces", []),
            app_url=state["app_url"],
            codebase_summary=state.get("codebase_summary", ""),
            changed_files=state.get("changed_files"),
            max_tests_per_surface=3,
        )

        if result.success and result.data:
            test_plan = result.data

            # Convert TestSpec objects to dicts for state storage
            test_plan_dicts = [test.to_dict() for test in test_plan.tests]

            # Sort by priority
            priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
            test_plan_dicts.sort(
                key=lambda t: priority_order.get(t.get("priority", "low"), 3)
            )

            state["test_plan"] = test_plan_dicts
            state["test_priorities"] = {t["id"]: t["priority"] for t in test_plan_dicts}
            state["current_test_index"] = 0
            state["coverage_summary"] = test_plan.coverage_summary

            # Estimate session config for the entire plan
            session_config = agent.estimate_session_config_for_plan(test_plan)
            state["session_config"] = session_config

            # Track usage
            state["total_input_tokens"] += result.input_tokens
            state["total_output_tokens"] += result.output_tokens
            state["total_cost"] += result.cost

            log.info(
                "Test plan created",
                total_tests=len(test_plan_dicts),
                estimated_duration_ms=test_plan.total_estimated_duration_ms,
                session_config=session_config,
            )
        else:
            log.error("TestPlannerAgent failed", error=result.error)
            state["error"] = f"Test planning failed: {result.error}"

    except Exception as e:
        log.error("Planning failed", error=str(e))
        state["error"] = f"Test planning failed: {str(e)}"

    return state


async def execute_test_node(state: TestingState) -> TestingState:
    """
    Execute the current test using Playwright for UI tests or HTTP for API tests.

    This node:
    1. Gets the current test from the plan
    2. Executes it using appropriate method (UI/API/DB)
    3. Records results and screenshots
    4. Queues failures for healing if needed
    """
    log = logger.bind(node="execute_test")

    # Get current test
    test_plan = state.get("test_plan", [])
    current_idx = state.get("current_test_index", 0)

    if current_idx >= len(test_plan):
        log.info("All tests completed")
        state["should_continue"] = False
        return state

    test = test_plan[current_idx]
    state["current_test"] = test

    log = log.bind(test_id=test["id"], test_name=test["name"])
    log.info("Executing test")

    time.time()
    settings = get_settings()

    if test["type"] == "ui":
        # Execute UI test with Playwright
        test_result = await _execute_ui_test(test, state["app_url"], settings, log)
    elif test["type"] == "api":
        # Execute API test with httpx
        test_result = await _execute_api_test(test, state["app_url"], settings, log)
    else:
        # Execute DB test or fall back to simulation
        test_result = await _execute_simulated_test(test, state, settings, log)

    # Update state with result
    state["test_results"].append(test_result.to_dict())

    if test_result.status == TestStatus.PASSED:
        state["passed_count"] += 1
        log.info("Test passed", duration=test_result.duration_seconds)
    else:
        state["failed_count"] += 1
        log.warning("Test failed", error=test_result.error_message)

        # Queue for healing
        state["healing_queue"].append(test["id"])
        state["failures"].append(FailureAnalysis(
            test_id=test["id"],
            failure_type="unknown",
            root_cause=test_result.error_message or "Unknown",
            confidence=0.0,
            screenshot_at_failure=test_result.screenshots[-1] if test_result.screenshots else None,
        ).to_dict())

    # Emit test execution events to Redpanda for downstream processing
    await _emit_test_events(state, test, test_result, log)

    # Move to next test
    state["current_test_index"] = current_idx + 1
    state["iteration"] += 1

    return state


async def _execute_ui_test(
    test: dict,
    app_url: str,
    settings,
    log,
) -> TestResult:
    """Execute a UI test using Cloudflare Worker via E2EBrowserClient.

    This connects to the deployed Cloudflare Worker which provides:
    - Multi-backend browser support (Cloudflare Browser + TestingBot)
    - Cross-browser testing (Chrome, Firefox, Safari, Edge)
    - Real device testing (iOS, Android)
    - AI-powered natural language actions
    - Self-healing selectors
    """
    import base64
    import os

    start_time = time.time()
    actions_taken = []
    screenshots = []
    assertions_passed = 0
    assertions_failed = 0
    error_message = None

    try:
        from ..browser.e2e_client import E2EBrowserClient

        # Connect to Cloudflare Worker
        worker_url = os.environ.get(
            "E2E_WORKER_URL",
            "https://e2e-testing-agent.samuelvinay-kumar.workers.dev"
        )

        async with E2EBrowserClient(endpoint=worker_url) as client:
            # Create page and navigate
            page = await client.new_page(app_url)

            # Execute test steps using natural language or structured actions
            for i, step in enumerate(test.get("steps", [])):
                step_start = time.time()
                action_type = step.get("action", "click")
                target = step.get("target")
                value = step.get("value")
                description = step.get("description", "")

                log.debug(f"Executing step {i+1}", action=action_type, target=target)

                try:
                    # Convert structured steps to natural language for AI execution
                    if action_type == "goto":
                        url = target if target.startswith("http") else f"{app_url}{target}"
                        result = await page.act(f"Navigate to {url}")
                    elif action_type == "click":
                        # Use description if available, otherwise use selector
                        instruction = description or f"Click the element {target}"
                        result = await page.act(instruction)
                    elif action_type == "fill":
                        instruction = description or f"Type '{value}' in the {target} field"
                        result = await page.act(instruction)
                    elif action_type == "type":
                        instruction = description or f"Type '{value}' in {target}"
                        result = await page.act(instruction)
                    elif action_type == "wait":
                        import asyncio
                        await asyncio.sleep(int(value or 1000) / 1000)
                        result = type('Result', (), {'success': True})()
                    elif action_type == "wait_for_selector":
                        result = await page.observe(f"Wait for {target} to appear")
                    elif action_type == "press":
                        instruction = f"Press {value or 'Enter'} key"
                        result = await page.act(instruction)
                    elif action_type == "select":
                        instruction = description or f"Select '{value}' from {target}"
                        result = await page.act(instruction)
                    elif action_type == "hover":
                        instruction = description or f"Hover over {target}"
                        result = await page.act(instruction)
                    elif action_type == "screenshot":
                        screenshot_bytes = await page.screenshot()
                        if screenshot_bytes:
                            screenshots.append(base64.b64encode(screenshot_bytes).decode())
                        result = type('Result', (), {'success': True})()
                    else:
                        # For unknown actions, try natural language
                        instruction = description or f"{action_type} {target} {value or ''}".strip()
                        result = await page.act(instruction)

                    if not getattr(result, 'success', True):
                        raise Exception(getattr(result, 'error', f"Step failed: {action_type}"))

                    actions_taken.append({
                        "step": i + 1,
                        "action": action_type,
                        "target": target,
                        "result": "success",
                        "duration_ms": (time.time() - step_start) * 1000,
                        "cached": getattr(result, 'cached', False),
                        "healed": getattr(result, 'healed', False),
                    })

                except Exception as step_error:
                    actions_taken.append({
                        "step": i + 1,
                        "action": action_type,
                        "target": target,
                        "result": "failure",
                        "error": str(step_error),
                    })
                    # Capture screenshot on failure
                    try:
                        screenshot_bytes = await page.screenshot()
                        if screenshot_bytes:
                            screenshots.append(base64.b64encode(screenshot_bytes).decode())
                    except Exception:
                        pass
                    raise step_error

            # Execute assertions using AI observation
            for assertion in test.get("assertions", []):
                assertion_type = assertion.get("type")
                target = assertion.get("target")
                expected = assertion.get("expected")

                try:
                    if assertion_type == "element_visible":
                        obs = await page.observe(f"Is the element {target} visible on the page?")
                        if not obs.success or "not visible" in str(obs.result).lower():
                            raise AssertionError(f"Element {target} not visible")
                    elif assertion_type == "text_contains":
                        obs = await page.observe(f"Does the element {target} contain the text '{expected}'?")
                        if not obs.success or "no" in str(obs.result).lower():
                            raise AssertionError(f"Text '{expected}' not found in {target}")
                    elif assertion_type == "url_matches":
                        state = await page.get_state()
                        if expected not in state.url:
                            raise AssertionError(f"URL does not contain '{expected}'")
                    elif assertion_type == "value_equals":
                        extraction = await page.extract({"value": "string"}, f"Get the value of {target}")
                        actual = extraction.result.get("value", "") if extraction.result else ""
                        if actual != expected:
                            raise AssertionError(f"Expected '{expected}', got '{actual}'")
                    elif assertion_type == "element_count":
                        obs = await page.observe(f"How many {target} elements are visible?")
                        # Simple check - in production would parse the number
                        assertions_passed += 1
                        continue

                    assertions_passed += 1

                except AssertionError as ae:
                    assertions_failed += 1
                    if not error_message:
                        error_message = str(ae)

            # Take final screenshot
            try:
                screenshot_bytes = await page.screenshot()
                if screenshot_bytes:
                    screenshots.append(base64.b64encode(screenshot_bytes).decode())
            except Exception:
                pass

        # Determine status
        status = TestStatus.PASSED if assertions_failed == 0 and not error_message else TestStatus.FAILED

    except Exception as e:
        log.error("UI test execution error", error=str(e))
        status = TestStatus.FAILED
        error_message = str(e)

    return TestResult(
        test_id=test["id"],
        status=status,
        duration_seconds=time.time() - start_time,
        error_message=error_message,
        screenshots=screenshots,
        actions_taken=actions_taken,
        assertions_passed=assertions_passed,
        assertions_failed=assertions_failed,
    )


async def _execute_api_test(
    test: dict,
    app_url: str,
    settings,
    log,
) -> TestResult:
    """Execute an API test using httpx."""
    import httpx

    start_time = time.time()
    actions_taken = []
    assertions_passed = 0
    assertions_failed = 0
    error_message = None

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            for i, step in enumerate(test.get("steps", [])):
                action = step.get("action", "api_call")

                if action == "api_call":
                    method = step.get("method", "GET").upper()
                    path = step.get("target", "/")
                    url = f"{app_url}{path}" if not path.startswith("http") else path
                    body = step.get("value")
                    headers = step.get("headers", {})

                    log.debug(f"API call {i+1}", method=method, url=url)

                    try:
                        if method == "GET":
                            response = await client.get(url, headers=headers)
                        elif method == "POST":
                            response = await client.post(url, json=body, headers=headers)
                        elif method == "PUT":
                            response = await client.put(url, json=body, headers=headers)
                        elif method == "DELETE":
                            response = await client.delete(url, headers=headers)
                        elif method == "PATCH":
                            response = await client.patch(url, json=body, headers=headers)
                        else:
                            raise ValueError(f"Unsupported HTTP method: {method}")

                        actions_taken.append({
                            "step": i + 1,
                            "action": action,
                            "method": method,
                            "url": url,
                            "status_code": response.status_code,
                            "result": "success" if response.is_success else "failure",
                        })

                        # Store response for assertions
                        step["_response"] = response

                    except Exception as e:
                        actions_taken.append({
                            "step": i + 1,
                            "action": action,
                            "method": method,
                            "url": url,
                            "result": "failure",
                            "error": str(e),
                        })
                        raise

            # Execute assertions
            for assertion in test.get("assertions", []):
                assertion_type = assertion.get("type")
                expected = assertion.get("expected")

                try:
                    # Get the last response for assertions
                    last_step = test.get("steps", [{}])[-1]
                    response = last_step.get("_response")

                    if not response:
                        raise AssertionError("No response to assert against")

                    if assertion_type == "status_code":
                        if response.status_code != int(expected):
                            raise AssertionError(
                                f"Status code mismatch: expected {expected}, got {response.status_code}"
                            )
                    elif assertion_type == "response_contains":
                        text = response.text
                        if expected not in text:
                            raise AssertionError(f"Response does not contain: {expected}")
                    elif assertion_type == "json_path":
                        # Simple JSON path check
                        target = assertion.get("target")
                        data = response.json()
                        value = data.get(target)
                        if str(value) != str(expected):
                            raise AssertionError(
                                f"JSON value mismatch at {target}: expected {expected}, got {value}"
                            )

                    assertions_passed += 1

                except AssertionError as ae:
                    assertions_failed += 1
                    if not error_message:
                        error_message = str(ae)

        status = TestStatus.PASSED if assertions_failed == 0 and not error_message else TestStatus.FAILED

    except Exception as e:
        log.error("API test execution error", error=str(e))
        status = TestStatus.FAILED
        error_message = str(e)

    return TestResult(
        test_id=test["id"],
        status=status,
        duration_seconds=time.time() - start_time,
        error_message=error_message,
        actions_taken=actions_taken,
        assertions_passed=assertions_passed,
        assertions_failed=assertions_failed,
    )


async def _execute_simulated_test(
    test: dict,
    state: TestingState,
    settings,
    log,
) -> TestResult:
    """Fall back to simulated test execution using Claude."""
    start_time = time.time()
    client = anthropic.Anthropic(api_key=settings.anthropic_api_key.get_secret_value())

    prompt = f"""You are executing a test. Analyze the test specification and provide realistic results.

TEST SPECIFICATION:
{json.dumps(test, indent=2)}

APP URL: {state["app_url"]}

Based on the test specification, determine if this test would likely pass or fail.
Consider realistic scenarios and potential issues.

Respond with JSON:
{{
    "status": "passed|failed",
    "steps_executed": [
        {{"step": 1, "action": "...", "result": "success|failure", "details": "..."}}
    ],
    "assertions_results": [
        {{"assertion": "...", "passed": true|false, "actual": "..."}}
    ],
    "error_message": null or "error description if failed"
}}
"""

    try:
        response = client.messages.create(
            model=settings.default_model.value,
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}],
        )

        # Parse result using robust parser
        content = response.content[0].text
        result = robust_json_parse(content)

        return TestResult(
            test_id=test["id"],
            status=TestStatus.PASSED if result["status"] == "passed" else TestStatus.FAILED,
            duration_seconds=time.time() - start_time,
            error_message=result.get("error_message"),
            actions_taken=result.get("steps_executed", []),
            assertions_passed=sum(1 for a in result.get("assertions_results", []) if a.get("passed")),
            assertions_failed=sum(1 for a in result.get("assertions_results", []) if not a.get("passed")),
        )

    except Exception as e:
        log.error("Simulated test execution error", error=str(e))
        return TestResult(
            test_id=test["id"],
            status=TestStatus.FAILED,
            duration_seconds=time.time() - start_time,
            error_message=str(e),
        )


async def self_heal_node(state: TestingState) -> TestingState:
    """
    Analyze failures and attempt to heal broken tests using the SelfHealerAgent.

    This node:
    1. Checks if healing is enabled in config
    2. Analyzes failures using the SelfHealerAgent (code-aware healing, memory store)
    3. Applies fixes if confidence is high enough
    4. Re-queues healed tests for retry

    The SelfHealerAgent provides:
    - Code-aware healing (git history + source analysis for 99.9% accuracy)
    - Cross-session learning via Cognee-powered memory store
    - Cached healing patterns for fast recovery
    - Hybrid retrieval (BM25 + Vector + Reranking)
    """
    log = logger.bind(node="self_heal")

    healing_queue = state.get("healing_queue", [])
    if not healing_queue:
        log.info("No tests to heal")
        return state

    settings = get_settings()

    # Check if healing is enabled
    if not settings.self_heal_enabled:
        log.info("Self-healing is disabled in settings")
        state["healing_queue"] = []
        return state

    # Import the SelfHealerAgent
    from ..agents.self_healer import SelfHealerAgent

    # Initialize the healer agent with configuration
    healer = SelfHealerAgent(
        auto_heal_threshold=settings.self_heal_confidence_threshold,
        repo_path=state.get("codebase_path", "."),
        enable_code_aware=True,
        enable_memory_store=True,
        enable_hybrid_retrieval=True,
        org_id=state.get("org_id"),
        project_id=state.get("project_id"),
    )

    # Track healed tests for retry
    healed_tests = []
    healed_test_specs = {}

    # Process each failure in the healing queue
    for test_id in list(healing_queue):
        test_log = logger.bind(node="self_heal", test_id=test_id)
        test_log.info("Attempting to heal test")

        # Find the failed test and its result
        test = next((t for t in state["test_plan"] if t["id"] == test_id), None)
        failure = next((f for f in state["failures"] if f["test_id"] == test_id), None)
        result = next((r for r in state["test_results"] if r["test_id"] == test_id), None)

        if not test:
            test_log.warning("Could not find test spec")
            continue

        # Build failure_details from failure analysis and result
        failure_details = {
            "type": failure.get("failure_type", "unknown") if failure else "unknown",
            "message": failure.get("root_cause", "") if failure else "",
            "error": result.get("error_message", "") if result else "",
            "selector": None,
            "step_index": 0,
            "url": state.get("app_url"),
            "failure_count": 1,
        }

        # Extract selector from failed step if available
        if result and result.get("actions_taken"):
            for i, action in enumerate(result["actions_taken"]):
                if action.get("result") == "failure":
                    failure_details["selector"] = action.get("target")
                    failure_details["step_index"] = i
                    failure_details["message"] = action.get("error", failure_details["message"])
                    break

        # Get screenshot at failure point
        screenshot = None
        if failure and failure.get("screenshot_at_failure"):
            import base64
            try:
                screenshot = base64.b64decode(failure["screenshot_at_failure"])
            except Exception:
                pass

        # Get error logs from result
        error_logs = None
        if result and result.get("error_message"):
            error_logs = result["error_message"]
            if result.get("actions_taken"):
                error_logs += "\n\nActions taken:\n"
                error_logs += json.dumps(result["actions_taken"], indent=2)

        try:
            # Call the SelfHealerAgent
            heal_result = await healer.execute(
                test_spec=test,
                failure_details=failure_details,
                screenshot=screenshot,
                error_logs=error_logs,
            )

            # Track token usage
            if heal_result.input_tokens or heal_result.output_tokens:
                state["total_input_tokens"] += heal_result.input_tokens
                state["total_output_tokens"] += heal_result.output_tokens
                # Calculate cost
                pricing = MODEL_PRICING.get(settings.default_model, MODEL_PRICING[settings.default_model])
                cost = (
                    heal_result.input_tokens * pricing["input"] / 1_000_000 +
                    heal_result.output_tokens * pricing["output"] / 1_000_000
                )
                state["total_cost"] += cost

            if heal_result.success and heal_result.data:
                healing_data = heal_result.data

                # Update failure analysis
                for f in state["failures"]:
                    if f["test_id"] == test_id:
                        f["failure_type"] = healing_data.diagnosis.failure_type.value
                        f["root_cause"] = healing_data.diagnosis.explanation
                        f["confidence"] = healing_data.diagnosis.confidence

                        # Add suggested fix
                        if healing_data.suggested_fixes:
                            best_fix = healing_data.suggested_fixes[0]
                            f["suggested_fix"] = best_fix.to_dict()

                        # Add code-aware context if available
                        if healing_data.diagnosis.code_context:
                            f["code_context"] = healing_data.diagnosis.code_context.to_dict()

                # Check if auto-healed
                if healing_data.auto_healed and healing_data.healed_test_spec:
                    test_log.info(
                        "Test auto-healed",
                        diagnosis=healing_data.diagnosis.failure_type.value,
                        confidence=healing_data.diagnosis.confidence,
                    )

                    # Update test plan with healed spec
                    for i, t in enumerate(state["test_plan"]):
                        if t["id"] == test_id:
                            state["test_plan"][i] = healing_data.healed_test_spec
                            break

                    # Mark for retry
                    healed_tests.append(test_id)
                    healed_test_specs[test_id] = healing_data.healed_test_spec

                    # Update result to show healing was applied
                    for r in state["test_results"]:
                        if r["test_id"] == test_id:
                            if healing_data.suggested_fixes:
                                r["healing_applied"] = healing_data.suggested_fixes[0].to_dict()
                            r["status"] = "healed"

                    # Record successful healing outcome if pattern was from memory
                    if hasattr(healing_data, '_memory_pattern_id'):
                        await healer._record_memory_outcome(
                            healing_data._memory_pattern_id,
                            success=True,  # Will be verified after retry
                        )
                else:
                    # Healing suggested but not auto-applied (requires approval or low confidence)
                    test_log.info(
                        "Healing suggested but not auto-applied",
                        diagnosis=healing_data.diagnosis.failure_type.value,
                        confidence=healing_data.diagnosis.confidence,
                        requires_approval=any(f.requires_review for f in healing_data.suggested_fixes),
                    )
            else:
                test_log.warning(
                    "Healing analysis failed",
                    error=heal_result.error,
                )

        except Exception as e:
            test_log.error("Healing failed", error=str(e))

    # Update state with healing results
    state["healing_queue"] = []  # Clear the queue
    state["iteration"] += 1

    # Store healed tests info for the graph to decide whether to re-run them
    if healed_tests:
        state["healed_tests"] = healed_tests
        state["healed_test_specs"] = healed_test_specs
        log.info(
            "Healing complete",
            healed_count=len(healed_tests),
            healed_tests=healed_tests,
        )
    else:
        state["healed_tests"] = []
        state["healed_test_specs"] = {}
        log.info("No tests were auto-healed")

    return state


async def prepare_healed_tests_node(state: TestingState) -> TestingState:
    """
    Prepare state for re-executing healed tests.

    This node sets up the state so that execute_test_node will re-run
    the tests that were successfully healed by SelfHealerAgent.

    It:
    1. Creates a retry queue from healed_tests
    2. Resets the test index to point to healed tests
    3. Clears the healed_tests list to prevent infinite loops
    """
    log = logger.bind(node="prepare_healed_tests")

    healed_tests = state.get("healed_tests", [])
    if not healed_tests:
        log.info("No healed tests to retry")
        return state

    log.info("Preparing to retry healed tests", count=len(healed_tests))

    # Find the indices of healed tests in the test plan
    test_plan = state.get("test_plan", [])
    healed_indices = []

    for i, test in enumerate(test_plan):
        if test.get("id") in healed_tests:
            healed_indices.append(i)

    if not healed_indices:
        log.warning("Could not find healed tests in test plan")
        state["healed_tests"] = []
        return state

    # Set up retry queue - we'll use a special field for retry
    state["retry_queue"] = healed_tests.copy()

    # Clear healed_tests to prevent re-processing
    state["healed_tests"] = []

    # Adjust counts - subtract the failed count since we're retrying
    # The actual pass/fail will be updated when execute_test_node runs
    state["failed_count"] = max(0, state["failed_count"] - len(healed_tests))

    log.info(
        "Ready to retry healed tests",
        retry_count=len(healed_tests),
        retry_indices=healed_indices,
    )

    return state


async def execute_healed_test_node(state: TestingState) -> TestingState:
    """
    Execute a single healed test from the retry queue.

    This is a variation of execute_test_node specifically for retrying
    tests that have been healed by SelfHealerAgent.
    """
    log = logger.bind(node="execute_healed_test")

    retry_queue = state.get("retry_queue", [])
    if not retry_queue:
        log.info("No more healed tests to retry")
        return state

    # Get the next test to retry
    test_id = retry_queue[0]
    state["retry_queue"] = retry_queue[1:]

    # Find the test spec (should be the healed version)
    test_plan = state.get("test_plan", [])
    test = next((t for t in test_plan if t.get("id") == test_id), None)

    if not test:
        log.warning("Could not find healed test spec", test_id=test_id)
        return state

    log = log.bind(test_id=test_id, test_name=test.get("name"))
    log.info("Retrying healed test")

    settings = get_settings()

    # Execute the test based on type
    if test.get("type") == "ui":
        test_result = await _execute_ui_test(test, state["app_url"], settings, log)
    elif test.get("type") == "api":
        test_result = await _execute_api_test(test, state["app_url"], settings, log)
    else:
        test_result = await _execute_simulated_test(test, state, settings, log)

    # Mark result as a retry after healing
    result_dict = test_result.to_dict()
    result_dict["is_healed_retry"] = True

    # Find and update the existing result for this test
    updated = False
    for i, r in enumerate(state["test_results"]):
        if r.get("test_id") == test_id:
            # Keep track of the original result
            result_dict["original_result"] = r
            state["test_results"][i] = result_dict
            updated = True
            break

    if not updated:
        state["test_results"].append(result_dict)

    # Update counts
    if test_result.status == TestStatus.PASSED:
        state["passed_count"] += 1
        log.info("Healed test passed", duration=test_result.duration_seconds)

        # Record healing success for learning
        log.info("Healing verified successful", test_id=test_id)
    else:
        state["failed_count"] += 1
        log.warning("Healed test still failing", error=test_result.error_message)

        # Don't re-queue for healing to prevent infinite loops
        # Just record that healing didn't work

    state["iteration"] += 1
    return state


async def report_node(state: TestingState) -> TestingState:
    """
    Generate final test report and notifications.

    This node:
    1. Summarizes all test results
    2. Creates reports (JSON, HTML, Markdown, JUnit)
    3. Sends notifications (Slack, GitHub PR comments)
    4. Saves all artifacts to disk
    """
    log = logger.bind(node="report")
    log.info("Generating report")

    settings = get_settings()

    # Calculate summary
    total_tests = state["passed_count"] + state["failed_count"] + state["skipped_count"]
    pass_rate = state["passed_count"] / total_tests if total_tests > 0 else 0

    # 1. Generate and save reports using the reporter module
    try:
        from ..integrations.reporter import create_report_from_state, create_reporter

        reporter = create_reporter(output_dir=settings.output_dir)
        report_data = create_report_from_state(state)
        report_paths = reporter.generate_all(report_data)

        log.info(
            "Reports saved",
            json=str(report_paths.get("json")),
            html=str(report_paths.get("html")),
        )

        # Store report paths in state
        state["report_paths"] = {k: str(v) for k, v in report_paths.items()}

    except Exception as e:
        log.error("Report generation failed", error=str(e))

    # 2. Send GitHub PR comment if PR number is provided
    if state.get("pr_number"):
        try:
            import os

            from ..integrations.github_integration import GitHubIntegration
            from ..integrations.github_integration import TestSummary as GHTestSummary

            github = GitHubIntegration()

            # Get repo info from environment or state
            owner = os.environ.get("GITHUB_REPOSITORY_OWNER", "")
            repo = os.environ.get("GITHUB_REPOSITORY", "").split("/")[-1] if os.environ.get("GITHUB_REPOSITORY") else ""

            if owner and repo:
                gh_summary = GHTestSummary(
                    total=total_tests,
                    passed=state["passed_count"],
                    failed=state["failed_count"],
                    skipped=state["skipped_count"],
                    duration_seconds=state["iteration"] * 2,  # Rough estimate
                    cost_usd=state["total_cost"],
                    failures=state["failures"],
                    screenshots=[],
                )

                await github.post_pr_comment(
                    owner=owner,
                    repo=repo,
                    pr_number=state["pr_number"],
                    summary=gh_summary,
                )
                log.info("Posted GitHub PR comment", pr=state["pr_number"])

        except Exception as e:
            log.warning("GitHub integration failed", error=str(e))

    # 3. Send Slack notification
    try:
        from ..integrations.slack_integration import SlackIntegration
        from ..integrations.slack_integration import TestSummary as SlackTestSummary

        slack = SlackIntegration()

        slack_summary = SlackTestSummary(
            total=total_tests,
            passed=state["passed_count"],
            failed=state["failed_count"],
            skipped=state["skipped_count"],
            duration_seconds=state["iteration"] * 2,
            cost_usd=state["total_cost"],
            failures=state["failures"],
        )

        await slack.send_test_results(slack_summary)
        log.info("Sent Slack notification")

    except Exception as e:
        log.warning("Slack integration failed", error=str(e))

    # 4. Generate AI summary for console output
    try:
        client = anthropic.Anthropic(api_key=settings.anthropic_api_key.get_secret_value())

        prompt = f"""Generate a brief (3-4 sentences) executive summary of this test run.

RESULTS:
- Total: {total_tests}, Passed: {state["passed_count"]}, Failed: {state["failed_count"]}
- Pass Rate: {pass_rate:.1%}
- Cost: ${state["total_cost"]:.4f}

FAILURES:
{json.dumps(state["failures"][:5], indent=2) if state["failures"] else "None"}

Focus on: overall health, critical issues, and next steps.
"""

        response = client.messages.create(
            model=settings.default_model.value,
            max_tokens=500,
            messages=[{"role": "user", "content": prompt}],
        )

        state = _track_usage(state, response)
        state["executive_summary"] = response.content[0].text

        log.info(
            "Report complete",
            total_tests=total_tests,
            passed=state["passed_count"],
            failed=state["failed_count"],
            pass_rate=f"{pass_rate:.1%}",
            cost=f"${state['total_cost']:.4f}",
        )

    except Exception as e:
        log.error("AI summary generation failed", error=str(e))

    state["should_continue"] = False
    return state


# ============================================================================
# QUALITY SUBGRAPH NODES
# ============================================================================


async def accessibility_check_node(state: TestingState) -> TestingState:
    """
    Check accessibility compliance for the application under test.

    This node:
    1. Instantiates the AccessibilityCheckerAgent
    2. Runs WCAG 2.1 Level AA compliance checks
    3. Stores results in the state for the quality report
    """
    log = logger.bind(node="accessibility_check")
    log.info("Running accessibility checks", url=state["app_url"])

    try:
        from ..agents.accessibility_checker import (
            AccessibilityCheckerAgent,
            WCAGLevel,
        )

        agent = AccessibilityCheckerAgent()
        result = await agent.execute(
            url=state["app_url"],
            wcag_level=WCAGLevel.AA,
            include_best_practices=True,
            test_keyboard=True,
        )

        if result.success and result.data:
            # Store accessibility results in state
            accessibility_data = {
                "url": result.data.url,
                "score": result.data.score,
                "wcag_level_achieved": result.data.wcag_level_achieved.value if result.data.wcag_level_achieved else None,
                "passes_wcag_aa": result.data.passes_wcag_aa(),
                "issues_count": len(result.data.issues),
                "critical_count": result.data.critical_count,
                "serious_count": result.data.serious_count,
                "issues": [issue.to_dict() for issue in result.data.issues[:20]],  # Limit to top 20
                "summary": result.data.summary,
                "recommendations": result.data.recommendations,
            }

            # Initialize quality_results if not present
            if "quality_results" not in state:
                state["quality_results"] = {}

            state["quality_results"]["accessibility"] = accessibility_data

            # Track usage
            state["total_input_tokens"] += result.input_tokens
            state["total_output_tokens"] += result.output_tokens
            state["total_cost"] += result.cost

            log.info(
                "Accessibility check complete",
                score=result.data.score,
                issues_found=len(result.data.issues),
                passes_aa=result.data.passes_wcag_aa(),
            )
        else:
            log.warning("Accessibility check failed", error=result.error)
            if "quality_results" not in state:
                state["quality_results"] = {}
            state["quality_results"]["accessibility"] = {
                "error": result.error or "Unknown error",
                "score": 0,
            }

    except Exception as e:
        log.exception("Accessibility check node error", error=str(e))
        if "quality_results" not in state:
            state["quality_results"] = {}
        state["quality_results"]["accessibility"] = {
            "error": str(e),
            "score": 0,
        }

    state["iteration"] += 1
    return state


async def performance_analysis_node(state: TestingState) -> TestingState:
    """
    Analyze application performance metrics.

    This node:
    1. Instantiates the PerformanceAnalyzerAgent
    2. Collects Core Web Vitals and other performance metrics
    3. Stores results in the state for the quality report
    """
    log = logger.bind(node="performance_analysis")
    log.info("Running performance analysis", url=state["app_url"])

    try:
        from ..agents.performance_analyzer import PerformanceAnalyzerAgent

        agent = PerformanceAnalyzerAgent()
        result = await agent.execute(
            url=state["app_url"],
            device="mobile",  # Mobile-first analysis
            include_trace=False,
        )

        if result.success and result.data:
            # Store performance results in state
            performance_data = {
                "url": result.data.url,
                "overall_grade": result.data.overall_grade.value,
                "summary": result.data.summary,
                "core_vitals": {
                    "lcp_ms": result.data.metrics.core_vitals.lcp_ms,
                    "lcp_grade": result.data.metrics.core_vitals.get_lcp_grade().value,
                    "fid_ms": result.data.metrics.core_vitals.fid_ms,
                    "fid_grade": result.data.metrics.core_vitals.get_fid_grade().value,
                    "cls": result.data.metrics.core_vitals.cls,
                    "cls_grade": result.data.metrics.core_vitals.get_cls_grade().value,
                },
                "issues": [
                    {
                        "category": issue.category,
                        "severity": issue.severity,
                        "title": issue.title,
                        "description": issue.description,
                        "savings_ms": issue.savings_ms,
                        "fix_suggestion": issue.fix_suggestion,
                    }
                    for issue in result.data.issues[:10]  # Limit to top 10
                ],
                "recommendations": result.data.recommendations,
            }

            # Initialize quality_results if not present
            if "quality_results" not in state:
                state["quality_results"] = {}

            state["quality_results"]["performance"] = performance_data

            # Track usage
            state["total_input_tokens"] += result.input_tokens
            state["total_output_tokens"] += result.output_tokens
            state["total_cost"] += result.cost

            log.info(
                "Performance analysis complete",
                grade=result.data.overall_grade.value,
                issues_found=len(result.data.issues),
            )
        else:
            log.warning("Performance analysis failed", error=result.error)
            if "quality_results" not in state:
                state["quality_results"] = {}
            state["quality_results"]["performance"] = {
                "error": result.error or "Unknown error",
                "overall_grade": "unknown",
            }

    except Exception as e:
        log.exception("Performance analysis node error", error=str(e))
        if "quality_results" not in state:
            state["quality_results"] = {}
        state["quality_results"]["performance"] = {
            "error": str(e),
            "overall_grade": "unknown",
        }

    state["iteration"] += 1
    return state


async def security_scan_node(state: TestingState) -> TestingState:
    """
    Scan the application for security vulnerabilities.

    This node:
    1. Instantiates the SecurityScannerAgent
    2. Runs OWASP Top 10 vulnerability checks
    3. Stores results in the state for the quality report
    """
    log = logger.bind(node="security_scan")
    log.info("Running security scan", url=state["app_url"])

    try:
        from ..agents.security_scanner import SecurityScannerAgent

        agent = SecurityScannerAgent()
        result = await agent.execute(
            url=state["app_url"],
            scan_type="standard",
            include_api_tests=True,
        )

        if result.success and result.data:
            # Store security results in state
            security_data = {
                "url": result.data.url,
                "risk_score": result.data.risk_score,
                "summary": result.data.summary,
                "scan_duration_ms": result.data.scan_duration_ms,
                "critical_count": result.data.critical_count,
                "high_count": result.data.high_count,
                "vulnerabilities": [v.to_dict() for v in result.data.vulnerabilities[:15]],  # Limit to top 15
                "missing_headers": result.data.headers.get_missing_headers(),
                "recommendations": result.data.recommendations,
            }

            # Initialize quality_results if not present
            if "quality_results" not in state:
                state["quality_results"] = {}

            state["quality_results"]["security"] = security_data

            # Track usage
            state["total_input_tokens"] += result.input_tokens
            state["total_output_tokens"] += result.output_tokens
            state["total_cost"] += result.cost

            log.info(
                "Security scan complete",
                risk_score=result.data.risk_score,
                vulnerabilities_found=len(result.data.vulnerabilities),
                critical=result.data.critical_count,
                high=result.data.high_count,
            )
        else:
            log.warning("Security scan failed", error=result.error)
            if "quality_results" not in state:
                state["quality_results"] = {}
            state["quality_results"]["security"] = {
                "error": result.error or "Unknown error",
                "risk_score": 0,
            }

    except Exception as e:
        log.exception("Security scan node error", error=str(e))
        if "quality_results" not in state:
            state["quality_results"] = {}
        state["quality_results"]["security"] = {
            "error": str(e),
            "risk_score": 0,
        }

    state["iteration"] += 1
    return state


async def quality_report_node(state: TestingState) -> TestingState:
    """
    Generate a comprehensive quality report from all quality checks.

    This node:
    1. Aggregates results from accessibility, performance, and security checks
    2. Uses AI to generate an executive summary
    3. Produces actionable recommendations
    """
    log = logger.bind(node="quality_report")
    log.info("Generating quality report")

    settings = get_settings()
    quality_results = state.get("quality_results", {})

    # Extract scores and key metrics
    accessibility = quality_results.get("accessibility", {})
    performance = quality_results.get("performance", {})
    security = quality_results.get("security", {})

    # Calculate overall quality score
    scores = []
    if accessibility.get("score") is not None and "error" not in accessibility:
        scores.append(accessibility["score"])
    if performance.get("overall_grade") and "error" not in performance:
        grade_scores = {"excellent": 95, "good": 80, "needs_work": 60, "poor": 30}
        scores.append(grade_scores.get(performance["overall_grade"], 50))
    if security.get("risk_score") is not None and "error" not in security:
        # Invert risk score (lower risk = higher quality)
        scores.append(max(0, 100 - security["risk_score"]))

    overall_score = sum(scores) / len(scores) if scores else 0

    # Generate AI summary
    try:
        client = anthropic.Anthropic(api_key=settings.anthropic_api_key.get_secret_value())

        summary_data = {
            "accessibility": {
                "score": accessibility.get("score", "N/A"),
                "passes_wcag_aa": accessibility.get("passes_wcag_aa", "N/A"),
                "issues": accessibility.get("issues_count", 0),
                "critical": accessibility.get("critical_count", 0),
            },
            "performance": {
                "grade": performance.get("overall_grade", "N/A"),
                "lcp_grade": performance.get("core_vitals", {}).get("lcp_grade", "N/A"),
                "cls_grade": performance.get("core_vitals", {}).get("cls_grade", "N/A"),
            },
            "security": {
                "risk_score": security.get("risk_score", "N/A"),
                "critical_vulnerabilities": security.get("critical_count", 0),
                "high_vulnerabilities": security.get("high_count", 0),
            },
        }

        prompt = f"""Generate an executive quality report summary for this application:

URL: {state["app_url"]}

QUALITY METRICS:
{json.dumps(summary_data, indent=2)}

OVERALL QUALITY SCORE: {overall_score:.0f}/100

Provide:
1. Executive summary (3-4 sentences covering overall quality posture)
2. Top 5 critical issues that need immediate attention
3. Recommendations for each quality dimension
4. Overall assessment (ship-ready, needs-work, critical-issues)

Respond with JSON:
{{
    "executive_summary": "<summary>",
    "overall_assessment": "ship-ready|needs-work|critical-issues",
    "critical_issues": ["<issue>"],
    "accessibility_recommendations": ["<rec>"],
    "performance_recommendations": ["<rec>"],
    "security_recommendations": ["<rec>"]
}}"""

        response = client.messages.create(
            model=settings.default_model.value,
            max_tokens=1500,
            messages=[{"role": "user", "content": prompt}],
        )

        state = _track_usage(state, response)

        # Parse response using robust parser
        content = response.content[0].text
        report_data = robust_json_parse(content)

        # Store quality report
        state["quality_report"] = {
            "overall_score": overall_score,
            "overall_assessment": report_data.get("overall_assessment", "needs-work"),
            "executive_summary": report_data.get("executive_summary", ""),
            "critical_issues": report_data.get("critical_issues", []),
            "recommendations": {
                "accessibility": report_data.get("accessibility_recommendations", []),
                "performance": report_data.get("performance_recommendations", []),
                "security": report_data.get("security_recommendations", []),
            },
            "details": {
                "accessibility": accessibility,
                "performance": performance,
                "security": security,
            },
        }

        log.info(
            "Quality report generated",
            overall_score=overall_score,
            assessment=report_data.get("overall_assessment"),
        )

    except Exception as e:
        log.exception("Quality report generation error", error=str(e))
        state["quality_report"] = {
            "overall_score": overall_score,
            "overall_assessment": "unknown",
            "executive_summary": f"Quality report generation failed: {str(e)}",
            "critical_issues": [],
            "recommendations": {},
            "details": {
                "accessibility": accessibility,
                "performance": performance,
                "security": security,
            },
        }

    state["iteration"] += 1
    return state


# ============================================================================
# FLAKY TEST DETECTION NODE
# ============================================================================


async def detect_flaky_tests_node(state: TestingState) -> TestingState:
    """
    Detect flaky tests after test execution.

    This node uses FlakyTestDetector to:
    1. Analyze test results for flakiness patterns
    2. Calculate flakiness scores and levels
    3. Identify likely causes (timing, network, data, etc.)
    4. Generate recommendations for fixing flaky tests
    5. Optionally quarantine highly flaky tests

    Should be called after execute_test_node completes all tests.
    """
    log = logger.bind(node="detect_flaky_tests")
    log.info("Analyzing tests for flakiness")

    try:
        from datetime import datetime

        from ..agents.flaky_detector import (
            FlakinessLevel,
            FlakyTestDetector,
            QuarantineConfig,
            TestRun,
        )

        # Create detector with configurable thresholds
        settings = get_settings()
        config = QuarantineConfig(
            auto_quarantine_threshold=getattr(settings, 'flaky_quarantine_threshold', 0.3),
            auto_restore_threshold=getattr(settings, 'flaky_restore_threshold', 0.95),
            min_runs_for_decision=getattr(settings, 'flaky_min_runs', 10),
            retry_on_failure=getattr(settings, 'flaky_retry_count', 3),
        )
        detector = FlakyTestDetector(config=config)

        # Load historical test runs if available
        historical_runs = state.get("test_history", {})
        for test_id, runs in historical_runs.items():
            for run_data in runs:
                detector.record_run(TestRun(
                    test_id=test_id,
                    passed=run_data.get("passed", False),
                    duration_ms=run_data.get("duration_ms", 0),
                    timestamp=datetime.fromisoformat(run_data.get("timestamp", datetime.utcnow().isoformat())),
                    error_message=run_data.get("error_message"),
                    environment=run_data.get("environment"),
                    retry_number=run_data.get("retry_number", 0),
                ))

        # Record current test results
        test_results = state.get("test_results", [])
        now = datetime.utcnow()

        for result in test_results:
            test_id = result.get("test_id", "")
            detector.record_run(TestRun(
                test_id=test_id,
                passed=result.get("status") == "passed",
                duration_ms=result.get("duration_seconds", 0) * 1000,
                timestamp=now,
                error_message=result.get("error_message"),
                environment=state.get("environment", "ci"),
                retry_number=result.get("retry_number", 0),
                ci_run_id=state.get("run_id"),
            ))

        # Analyze each test for flakiness
        flaky_analysis = []
        quarantine_candidates = []
        retry_candidates = []

        for result in test_results:
            test_id = result.get("test_id", "")
            test_name = result.get("test_name", test_id)

            report = detector.analyze_test(test_id, test_name)

            analysis_entry = {
                "test_id": test_id,
                "test_name": test_name,
                "flakiness_level": report.flakiness_level.value,
                "flakiness_score": round(report.flakiness_score, 3),
                "pass_rate": round(report.pass_rate, 3),
                "total_runs": report.total_runs,
                "recent_failures": report.recent_failures,
                "likely_cause": report.likely_cause.value,
                "cause_confidence": round(report.cause_confidence, 2),
                "recommended_action": report.recommended_action,
                "should_quarantine": report.should_quarantine,
                "failure_patterns": report.failure_patterns,
            }
            flaky_analysis.append(analysis_entry)

            # Track tests that need quarantine
            if report.should_quarantine:
                quarantine_candidates.append(test_id)
                detector.quarantine_test(test_id)

            # Track tests that should be retried
            if report.flakiness_level != FlakinessLevel.STABLE:
                if result.get("status") != "passed":
                    retry_candidates.append(test_id)

        # Generate summary report
        flaky_report = detector.get_flaky_tests_report()

        # Store results in state
        state["flaky_analysis"] = flaky_analysis
        state["flaky_report"] = flaky_report
        state["quarantined_tests"] = quarantine_candidates
        state["retry_candidates"] = retry_candidates

        log.info(
            "Flaky test detection complete",
            total_tests=len(test_results),
            flaky_tests=flaky_report.get("flaky_tests", 0),
            quarantined=len(quarantine_candidates),
            retry_candidates=len(retry_candidates),
        )

    except Exception as e:
        log.exception("Flaky test detection failed", error=str(e))
        state["flaky_analysis"] = {"error": str(e)}

    state["iteration"] += 1
    return state


# ============================================================================
# TEST IMPACT ANALYSIS NODE
# ============================================================================


async def analyze_test_impact_node(state: TestingState) -> TestingState:
    """
    Analyze code changes to determine which tests should run.

    This node uses TestImpactAnalyzer to:
    1. Analyze code changes (from PR or commit)
    2. Identify affected tests based on dependencies
    3. Calculate risk scores for the changes
    4. Identify coverage gaps
    5. Suggest new tests to fill gaps
    6. Provide intelligent test selection

    Should be called early in the pipeline when changed_files are available.
    This enables 10-100x faster CI/CD by running only affected tests.
    """
    log = logger.bind(node="analyze_test_impact")

    # Only run impact analysis if we have changed files
    changed_files = state.get("changed_files", [])
    if not changed_files:
        log.info("No changed files, skipping impact analysis")
        return state

    log.info("Analyzing test impact for changed files", files_count=len(changed_files))

    try:
        from datetime import datetime

        from ..agents.test_impact_analyzer import (
            CodeChange,
            SmartTestSelector,
            TestImpactAnalyzer,
        )

        analyzer = TestImpactAnalyzer()

        # Create CodeChange object from state
        change = CodeChange(
            id=state.get("pr_number") or state.get("commit_sha") or state.get("run_id", "unknown"),
            files=[
                {
                    "path": f,
                    "additions": 0,  # Would be populated from git diff
                    "deletions": 0,
                    "patch": "",
                }
                for f in changed_files
            ],
            message=state.get("commit_message", ""),
            author=state.get("author", "unknown"),
            timestamp=datetime.utcnow(),
            branch=state.get("branch", "main"),
        )

        # Get all tests from test plan if available, or use testable surfaces
        all_tests = []
        if state.get("test_plan"):
            all_tests = [
                {
                    "id": t.get("id", f"test-{i}"),
                    "name": t.get("name", ""),
                    "category": t.get("type", "unknown"),
                    "priority": t.get("priority", "medium"),
                    "avg_duration": t.get("estimated_duration_ms", 30000) / 1000,
                }
                for i, t in enumerate(state["test_plan"])
            ]
        elif state.get("testable_surfaces"):
            all_tests = [
                {
                    "id": f"surface-{i}",
                    "name": s.get("name", ""),
                    "category": s.get("type", "unknown"),
                    "priority": s.get("priority", "medium"),
                    "avg_duration": 30,
                }
                for i, s in enumerate(state["testable_surfaces"])
            ]

        # Run impact analysis
        impact_result = await analyzer.analyze_impact(
            change=change,
            all_tests=all_tests,
            coverage_data=state.get("coverage_data"),
        )

        # Store impact analysis results
        state["impact_analysis"] = {
            "change_id": impact_result.change_id,
            "affected_tests": impact_result.affected_tests,
            "unaffected_tests": impact_result.unaffected_tests,
            "risk_score": round(impact_result.risk_score, 2),
            "estimated_time_saved": round(impact_result.estimated_time_saved, 1),
            "coverage_gaps": impact_result.coverage_gaps,
            "recommendations": impact_result.recommendations,
            "new_tests_suggested": impact_result.new_tests_suggested,
        }

        # Use SmartTestSelector for intelligent test selection
        selector = SmartTestSelector(analyzer)
        settings = get_settings()
        time_budget = getattr(settings, 'test_time_budget', None)
        risk_tolerance = getattr(settings, 'test_risk_tolerance', 'medium')

        selection = await selector.select_tests(
            change=change,
            all_tests=all_tests,
            time_budget_seconds=time_budget,
            risk_tolerance=risk_tolerance,
        )

        state["test_selection"] = {
            "must_run": selection["must_run"],
            "should_run": selection["should_run"],
            "can_skip": selection["can_skip"],
            "deferred": selection["deferred"],
            "estimated_time": round(selection["estimated_time"], 1),
            "coverage_estimate": round(selection["coverage_estimate"], 2),
        }

        # If impact analysis suggests skipping some tests, filter test plan
        if state.get("test_plan") and selection["can_skip"]:
            original_count = len(state["test_plan"])
            must_run_ids = set(selection["must_run"] + selection["should_run"])

            # Filter test plan to only run affected tests
            if getattr(settings, 'use_test_impact_filtering', True):
                state["test_plan"] = [
                    t for t in state["test_plan"]
                    if t.get("id") in must_run_ids
                ]
                state["skipped_tests_by_impact"] = selection["can_skip"]

                log.info(
                    "Test plan filtered by impact analysis",
                    original_count=original_count,
                    filtered_count=len(state["test_plan"]),
                    skipped_count=len(selection["can_skip"]),
                )

        log.info(
            "Test impact analysis complete",
            affected_tests=len(impact_result.affected_tests),
            unaffected_tests=len(impact_result.unaffected_tests),
            risk_score=impact_result.risk_score,
            time_saved_seconds=impact_result.estimated_time_saved,
        )

    except Exception as e:
        log.exception("Test impact analysis failed", error=str(e))
        state["impact_analysis"] = {"error": str(e)}

    state["iteration"] += 1
    return state